{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7575d80e",
   "metadata": {},
   "source": [
    "\n",
    "Generative Models: Investigating **VAE** vs **GAN** Biases (CIFAR-10)\n",
    "\n",
    "This notebook trains and evaluates a **Variational Autoencoder (VAE)** and a **DCGAN** on **CIFAR-10**, then compares their biases:\n",
    "- **Fidelity vs. Diversity**\n",
    "- **Reconstruction vs. Generation**\n",
    "- **Latent Space Structure & Interpolations**\n",
    "- **Out-of-Distribution (OOD) behavior**\n",
    "- **Training dynamics & stability**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-fidelity==0.3.0\n",
      "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.13/site-packages (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.13/site-packages (from torch-fidelity==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/lib/python3.13/site-packages (from torch-fidelity==0.3.0) (11.3.0)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.13/site-packages (from torch-fidelity==0.3.0) (2.7.0)\n",
      "Collecting torchvision (from torch-fidelity==0.3.0)\n",
      "  Using cached torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (from torch-fidelity==0.3.0) (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.13/site-packages (from torch->torch-fidelity==0.3.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/miniconda3/lib/python3.13/site-packages (from torch->torch-fidelity==0.3.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch->torch-fidelity==0.3.0) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.13/site-packages (from torch->torch-fidelity==0.3.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.13/site-packages (from torch->torch-fidelity==0.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch->torch-fidelity==0.3.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.13/site-packages (from torch->torch-fidelity==0.3.0) (2025.3.2)\n",
      "Collecting torch (from torch-fidelity==0.3.0)\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch->torch-fidelity==0.3.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch->torch-fidelity==0.3.0) (3.0.2)\n",
      "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Using cached torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/73.6 MB\u001b[0m \u001b[31m60.7 kB/s\u001b[0m eta \u001b[36m0:17:42\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!python -m pip install torch-fidelity==0.3.0 scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc62d33",
   "metadata": {},
   "source": [
    "## Imports & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, math, random, numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "try:\n",
    "    from torch_fidelity import calculate_metrics as tf_calculate_metrics\n",
    "    TORCH_FIDELITY_OK = True\n",
    "except Exception:\n",
    "    TORCH_FIDELITY_OK = False\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def save_grid(tensor, path, nrow=8, normalize=True, value_range=(-1,1)):\n",
    "    grid = vutils.make_grid(tensor, nrow=nrow, normalize=normalize, value_range=value_range)\n",
    "    plt.figure(figsize=(8,8)); plt.axis('off')\n",
    "    plt.imshow(np.transpose(grid.cpu().numpy(), (1,2,0)))\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "def save_fig(path):\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a2733",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a00c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Cfg:\n",
    "    run = 'both'          # 'vae', 'gan', or 'both'\n",
    "    epochs = 30\n",
    "    batch_size = 128\n",
    "    z_dim = 128\n",
    "    beta = 1.0          \n",
    "    recon_loss = 'mse'    \n",
    "    lr_vae = 1e-3\n",
    "    lr_g = 2e-4\n",
    "    lr_d = 2e-4\n",
    "    seed = 42\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    outdir = None        \n",
    "    compute_metrics = False \n",
    "    run_ood = False         \n",
    "\n",
    "set_seed(Cfg.seed)\n",
    "stamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "OUTDIR = Path(Cfg.outdir or f'./runs/{stamp}')\n",
    "(OUTDIR / 'checkpoints').mkdir(parents=True, exist_ok=True)\n",
    "(OUTDIR / 'figures').mkdir(parents=True, exist_ok=True)\n",
    "DEVICE = torch.device(Cfg.device)\n",
    "OUTDIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa1c1c",
   "metadata": {},
   "source": [
    "## Data Loaders (CIFAR-10, plus OOD sets if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f5582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataloaders(batch_size=128, num_workers=2, root='./data'):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "    train = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=tfm)\n",
    "    test  = torchvision.datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_ood_loaders(batch_size=128, num_workers=2, root='./data'):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "    cifar100 = torchvision.datasets.CIFAR100(root=root, train=False, download=True, transform=tfm)\n",
    "    svhn = torchvision.datasets.SVHN(root=root, split='test', download=True, transform=tfm)\n",
    "    cifar100_loader = DataLoader(cifar100, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    svhn_loader = DataLoader(svhn, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return cifar100_loader, svhn_loader\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(Cfg.batch_size)\n",
    "try:\n",
    "    next(iter(train_loader))\n",
    "    print(\"CIFAR-10 ready.\")\n",
    "except StopIteration:\n",
    "    print(\"Data issue.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156eaa7",
   "metadata": {},
   "source": [
    "## VAE: Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256*4*4, Cfg.z_dim)\n",
    "        self.fc_logvar = nn.Linear(256*4*4, Cfg.z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x).view(x.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(Cfg.z_dim, 256*4*4)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),   \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z).view(z.size(0), 256, 4, 4)\n",
    "        return self.net(h)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar, z\n",
    "\n",
    "def vae_loss(x, x_recon, mu, logvar, recon_type='mse', beta=1.0):\n",
    "    if recon_type == 'mse':\n",
    "        recon = F.mse_loss(x_recon, x, reduction='sum') / x.size(0)\n",
    "    else:\n",
    "        x_scaled = (x + 1) / 2\n",
    "        x_recon_scaled = (x_recon + 1) / 2\n",
    "        recon = F.binary_cross_entropy(x_recon_scaled, x_scaled, reduction='sum') / x.size(0)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon + beta * kl, recon, kl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded57513",
   "metadata": {},
   "source": [
    "### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_vae():\n",
    "    vae = VAE(Cfg.z_dim).to(DEVICE)\n",
    "    opt = optim.Adam(vae.parameters(), lr=Cfg.lr_vae, betas=(0.9, 0.999))\n",
    "    history = {'loss': [], 'recon': [], 'kl': []}\n",
    "    fixed_test = next(iter(test_loader))[0][:64].to(DEVICE)\n",
    "\n",
    "    for epoch in range(1, Cfg.epochs+1):\n",
    "        vae.train()\n",
    "        total_loss = total_recon = total_kl = 0.0\n",
    "        for x, _ in tqdm(train_loader, desc=f'VAE Epoch {epoch}/{Cfg.epochs}'):\n",
    "            x = x.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            x_recon, mu, logvar, _ = vae(x)\n",
    "            loss, recon, kl = vae_loss(x, x_recon, mu, logvar, recon_type=Cfg.recon_loss, beta=Cfg.beta)\n",
    "            loss.backward(); opt.step()\n",
    "            b = x.size(0)\n",
    "            total_loss += loss.item() * b\n",
    "            total_recon += recon.item() * b\n",
    "            total_kl += kl.item() * b\n",
    "\n",
    "        n = len(train_loader.dataset)\n",
    "        epoch_loss = total_loss / n\n",
    "        epoch_recon = total_recon / n\n",
    "        epoch_kl = total_kl / n\n",
    "        history['loss'].append(epoch_loss); history['recon'].append(epoch_recon); history['kl'].append(epoch_kl)\n",
    "\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            x = fixed_test\n",
    "            x_recon, _, _, _ = vae(x)\n",
    "            save_grid(x.cpu(), OUTDIR / 'figures' / f\"vae_epoch{epoch:03d}_orig.png\")\n",
    "            save_grid(x_recon.cpu(), OUTDIR / 'figures' / f\"vae_epoch{epoch:03d}_recon.png\")\n",
    "            z = torch.randn(64, Cfg.z_dim, device=DEVICE)\n",
    "            samples = vae.decoder(z)\n",
    "            save_grid(samples.cpu(), OUTDIR / 'figures' / f\"vae_epoch{epoch:03d}_samples.png\")\n",
    "\n",
    "        torch.save(vae.state_dict(), OUTDIR / 'checkpoints' / f\"vae_epoch{epoch:03d}.pt\")\n",
    "    plt.figure()\n",
    "    plt.plot(history['loss'], label='loss'); plt.plot(history['recon'], label='recon'); plt.plot(history['kl'], label='kl')\n",
    "    plt.legend(); save_fig(OUTDIR / 'figures' / \"vae_training_curves.png\")\n",
    "    torch.save(vae.state_dict(), OUTDIR / 'checkpoints' / 'vae_final.pt')\n",
    "    return vae, history\n",
    "\n",
    "vae, vae_hist = (None, None)\n",
    "if Cfg.run in ['vae', 'both']:\n",
    "    vae, vae_hist = train_vae()\n",
    "    print(\"VAE trained and saved.\")\n",
    "else:\n",
    "    print(\"Skipping VAE training (Cfg.run != 'vae'/'both').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae612cf",
   "metadata": {},
   "source": [
    "## DCGAN: Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35414e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DCGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=128, ngf=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, ngf*4, 4, 1, 0, bias=False), \n",
    "            nn.BatchNorm2d(ngf*4), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False), \n",
    "            nn.BatchNorm2d(ngf*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),    \n",
    "            nn.BatchNorm2d(ngf), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),        \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z.view(z.size(0), z.size(1), 1, 1))\n",
    "\n",
    "class DCGANDiscriminator(nn.Module):\n",
    "    def __init__(self, ndf=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False), \n",
    "            nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf*4, 1, 4, 1, 0, bias=False),  \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1, 1).squeeze(1)\n",
    "\n",
    "def train_gan():\n",
    "    netG = DCGANGenerator(Cfg.z_dim).to(DEVICE)\n",
    "    netD = DCGANDiscriminator().to(DEVICE)\n",
    "    optG = optim.Adam(netG.parameters(), lr=Cfg.lr_g, betas=(0.5, 0.999))\n",
    "    optD = optim.Adam(netD.parameters(), lr=Cfg.lr_d, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    fixed_z = torch.randn(64, Cfg.z_dim, device=DEVICE)\n",
    "    history = {'d_loss': [], 'g_loss': []}\n",
    "\n",
    "    for epoch in range(1, Cfg.epochs+1):\n",
    "        g_loss_epoch = d_loss_epoch = 0.0\n",
    "        for x, _ in tqdm(train_loader, desc=f'GAN Epoch {epoch}/{Cfg.epochs}'):\n",
    "            x = x.to(DEVICE); b = x.size(0)\n",
    "\n",
    "            optD.zero_grad()\n",
    "            real_logits = netD(x); real_labels = torch.ones(b, device=DEVICE)\n",
    "            d_real = criterion(real_logits, real_labels)\n",
    "            z = torch.randn(b, Cfg.z_dim, device=DEVICE)\n",
    "            fake = netG(z)\n",
    "            fake_logits = netD(fake.detach()); fake_labels = torch.zeros(b, device=DEVICE)\n",
    "            d_fake = criterion(fake_logits, fake_labels)\n",
    "            d_loss = d_real + d_fake; d_loss.backward(); optD.step()\n",
    "\n",
    "            optG.zero_grad()\n",
    "            fake_logits = netD(fake)\n",
    "            g_loss = criterion(fake_logits, real_labels)\n",
    "            g_loss.backward(); optG.step()\n",
    "\n",
    "            d_loss_epoch += d_loss.item() * b\n",
    "            g_loss_epoch += g_loss.item() * b\n",
    "\n",
    "        n = len(train_loader.dataset)\n",
    "        history['d_loss'].append(d_loss_epoch / n)\n",
    "        history['g_loss'].append(g_loss_epoch / n)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            samples = netG(fixed_z)\n",
    "        save_grid(samples.cpu(), OUTDIR / 'figures' / f\"gan_epoch{epoch:03d}_samples.png\")\n",
    "        torch.save({'G': netG.state_dict(), 'D': netD.state_dict()}, OUTDIR / 'checkpoints' / f\"gan_epoch{epoch:03d}.pt\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history['d_loss'], label='D loss'); plt.plot(history['g_loss'], label='G loss')\n",
    "    plt.legend(); save_fig(OUTDIR / 'figures' / \"gan_training_curves.png\")\n",
    "    torch.save({'G': netG.state_dict(), 'D': netD.state_dict()}, OUTDIR / 'checkpoints' / 'gan_final.pt')\n",
    "    return netG, netD, history\n",
    "\n",
    "netG = netD = gan_hist = None\n",
    "if Cfg.run in ['gan', 'both']:\n",
    "    netG, netD, gan_hist = train_gan()\n",
    "    print(\"GAN trained and saved.\")\n",
    "else:\n",
    "    print(\"Skipping GAN training (Cfg.run != 'gan'/'both').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257eace",
   "metadata": {},
   "source": [
    "## Reconstructions (VAE) & Samples / Interpolations (Both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e296ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def vae_reconstructions(vae, loader, device, outdir):\n",
    "    vae.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(device)[:64]\n",
    "    x_recon, mu, logvar, z = vae(x)\n",
    "    save_grid(x.cpu(), outdir / \"vae_recon_orig.png\")\n",
    "    save_grid(x_recon.cpu(), outdir / \"vae_recon_recon.png\")\n",
    "    mse = F.mse_loss(x_recon, x, reduction='mean').item()\n",
    "    return mse\n",
    "\n",
    "@torch.no_grad()\n",
    "def vae_interpolations(vae, loader, device, outdir, steps=8):\n",
    "    vae.eval()\n",
    "    x, _ = next(iter(loader))\n",
    "    x1, x2 = x[:1].to(device), x[1:2].to(device)\n",
    "    mu1, logvar1 = vae.encoder(x1); mu2, logvar2 = vae.encoder(x2)\n",
    "    z1, z2 = mu1, mu2\n",
    "    alphas = torch.linspace(0,1,steps, device=device).view(-1,1)\n",
    "    z_interp = (1 - alphas) * z1 + alphas * z2\n",
    "    imgs = vae.decoder(z_interp)\n",
    "    save_grid(imgs.cpu(), outdir / \"vae_interp.png\", nrow=steps)\n",
    "\n",
    "@torch.no_grad()\n",
    "def gan_interpolations(netG, device, outdir, z_dim=128, steps=8):\n",
    "    if netG is None: return\n",
    "    netG.eval()\n",
    "    z1 = torch.randn(1, z_dim, device=device); z2 = torch.randn(1, z_dim, device=device)\n",
    "    alphas = torch.linspace(0,1,steps, device=device).view(-1,1)\n",
    "    z = (1 - alphas) * z1 + alphas * z2\n",
    "    imgs = netG(z)\n",
    "    save_grid(imgs.cpu(), outdir / \"gan_interp.png\", nrow=steps)\n",
    "\n",
    "metrics = {}\n",
    "if vae is not None:\n",
    "    recon_mse = vae_reconstructions(vae, test_loader, DEVICE, OUTDIR / 'figures')\n",
    "    vae_interpolations(vae, test_loader, DEVICE, OUTDIR / 'figures')\n",
    "    metrics['vae_recon_mse'] = float(recon_mse)\n",
    "gan_interpolations(netG, DEVICE, OUTDIR / 'figures', z_dim=Cfg.z_dim)\n",
    "print(\"Saved reconstructions and interpolations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dded96",
   "metadata": {},
   "source": [
    "## VAE Latent Space Analysis (PCA / optional t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def vae_latent_analysis(vae, loader, device, outdir, max_samples=5000):\n",
    "    if vae is None: return\n",
    "    vae.eval()\n",
    "    zs, ys, count = [], [], 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        mu, logvar = vae.encoder(x)\n",
    "        zs.append(mu.cpu().numpy()); ys.append(y.numpy())\n",
    "        count += x.size(0)\n",
    "        if count >= max_samples: break\n",
    "    Z = np.concatenate(zs, axis=0); Y = np.concatenate(ys, axis=0)\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    Zp = pca.fit_transform(Z)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    scatter = plt.scatter(Zp[:,0], Zp[:,1], c=Y, s=5, alpha=0.7)\n",
    "    plt.title(\"VAE latent PCA (CIFAR-10 classes)\")\n",
    "    plt.legend(*scatter.legend_elements(num_classes=10), title=\"Class\", loc=\"best\", fontsize=6)\n",
    "    save_fig(outdir / \"vae_latent_pca.png\")\n",
    "    if SKLEARN_OK:\n",
    "        tsne = TSNE(n_components=2, init='pca', learning_rate='auto', perplexity=30, n_iter=1000)\n",
    "        Zt = tsne.fit_transform(Z[:3000])\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(Zt[:,0], Zt[:,1], c=Y[:3000], s=5, alpha=0.7)\n",
    "        plt.title(\"VAE latent t-SNE (CIFAR-10)\")\n",
    "        save_fig(outdir / \"vae_latent_tsne.png\")\n",
    "\n",
    "vae_latent_analysis(vae, test_loader, DEVICE, OUTDIR / 'figures')\n",
    "print(\"Saved latent projections.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ee5e3",
   "metadata": {},
   "source": [
    "## OOD: VAE Reconstruction Error (CIFAR-100 / SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def vae_ood(vae, in_loader, ood_loader, device, outdir, name='cifar10_vs_cifar100'):\n",
    "    if vae is None: return None\n",
    "    vae.eval()\n",
    "    def recon_errors(loader, max_batches=100):\n",
    "        errs = []\n",
    "        for bi, (x, _) in enumerate(loader):\n",
    "            x = x.to(device)\n",
    "            x_recon, mu, logvar, _ = vae(x)\n",
    "            mse = F.mse_loss(x_recon, x, reduction='none').view(x.size(0), -1).mean(dim=1)\n",
    "            errs.append(mse.cpu().numpy())\n",
    "            if bi+1 >= max_batches: break\n",
    "        return np.concatenate(errs, axis=0)\n",
    "\n",
    "    in_err = recon_errors(in_loader); ood_err = recon_errors(ood_loader)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(in_err, bins=50, alpha=0.7, label='In (CIFAR-10)', density=True)\n",
    "    plt.hist(ood_err, bins=50, alpha=0.7, label='OOD', density=True)\n",
    "    plt.xlabel('Reconstruction MSE per image'); plt.ylabel('Density')\n",
    "    plt.title('VAE Reconstruction Error: In vs OOD'); plt.legend()\n",
    "    save_fig(outdir / f\"vae_ood_{name}.png\")\n",
    "\n",
    "    from scipy.stats import rankdata\n",
    "    scores = np.concatenate([in_err, ood_err])\n",
    "    labels = np.concatenate([np.zeros_like(in_err), np.ones_like(ood_err)])\n",
    "    ranks = rankdata(scores)\n",
    "    pos = labels == 1; n_pos = pos.sum(); n_neg = (~pos).sum()\n",
    "    auc = (ranks[pos].sum() - n_pos*(n_pos+1)/2) / (n_pos*n_neg)\n",
    "    with open(outdir / f\"vae_ood_{name}.json\", 'w') as f:\n",
    "        json.dump({'auroc': float(auc)}, f, indent=2)\n",
    "    print(f\"[OOD] {name} AUROC ~ {auc:.3f}\")\n",
    "    return float(auc)\n",
    "\n",
    "if Cfg.run_ood and vae is not None:\n",
    "    cifar100_loader, svhn_loader = get_ood_loaders(Cfg.batch_size)\n",
    "    auc1 = vae_ood(vae, test_loader, cifar100_loader, DEVICE, OUTDIR / 'figures', name='cifar10_vs_cifar100')\n",
    "    auc2 = vae_ood(vae, test_loader, svhn_loader, DEVICE, OUTDIR / 'figures', name='cifar10_vs_svhn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76face09",
   "metadata": {},
   "source": [
    "## FID & Inception Score for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7968d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def dump_gan_samples(netG, outdir, device, z_dim=128, n=50000, batch=250):\n",
    "    if netG is None: return None\n",
    "    netG.eval()\n",
    "    save_dir = outdir / \"gan_samples_for_metrics\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    idx = 0\n",
    "    for _ in range(0, n, batch):\n",
    "        cur = min(batch, n - idx)\n",
    "        z = torch.randn(cur, z_dim, device=device)\n",
    "        imgs = netG(z).cpu()\n",
    "        imgs = (imgs + 1) / 2\n",
    "        for i in range(imgs.size(0)):\n",
    "            torchvision.utils.save_image(imgs[i], save_dir / f\"{idx+i:06d}.png\")\n",
    "        idx += cur\n",
    "    return save_dir\n",
    "\n",
    "def compute_fid_is(sample_dir, ref='cifar10', device='cuda'):\n",
    "    if not TORCH_FIDELITY_OK:\n",
    "        print(\"[Metrics] torch-fidelity not installed; skipping FID/IS.\"); return {}\n",
    "    mets = tf_calculate_metrics(input1=str(sample_dir), input2=ref, cuda=('cuda' in device), isc=True, fid=True, verbose=False)\n",
    "    return {k: float(v) for k, v in mets.items()}\n",
    "\n",
    "if Cfg.compute_metrics and netG is not None:\n",
    "    sample_dir = dump_gan_samples(netG, OUTDIR / 'figures', DEVICE, z_dim=Cfg.z_dim, n=50000, batch=250)\n",
    "    mets = compute_fid_is(sample_dir, ref='cifar10', device=Cfg.device)\n",
    "    metrics['gan_metrics'] = mets\n",
    "    print(mets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882603d9",
   "metadata": {},
   "source": [
    "## Save Summary & Where to Find Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1905b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(OUTDIR / 'results_summary.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Done. Find outputs in:\", str(OUTDIR.resolve()))\n",
    "print(\"Figures:\", str((OUTDIR / 'figures').resolve()))\n",
    "print(\"Checkpoints:\", str((OUTDIR / 'checkpoints').resolve()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
